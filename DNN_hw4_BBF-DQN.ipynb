{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Bigger, Better, Faster or Rainbow DQN v2**"
      ],
      "metadata": {
        "id": "KJhx0vRuV5hK"
      },
      "id": "KJhx0vRuV5hK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro:"
      ],
      "metadata": {
        "id": "g6t0j59HZK9q"
      },
      "id": "g6t0j59HZK9q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework description:"
      ],
      "metadata": {
        "id": "S7Ug_DKpfnqL"
      },
      "id": "S7Ug_DKpfnqL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction:"
      ],
      "metadata": {
        "id": "TYnn3zYlW6jb"
      },
      "id": "TYnn3zYlW6jb"
    },
    {
      "cell_type": "markdown",
      "id": "91a0c19f",
      "metadata": {
        "id": "91a0c19f"
      },
      "source": [
        "#### by: Mateusz Doliński, Mateusz Olko\n",
        "#### special thanks for the inspiration: Michał Nauman\n",
        "\n",
        "In this homework we will expand upon on the Deep Q-Network (DQN) algorithm [(Mnih 2014)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf). DQN has been successfully applied to a wide range of environments and has demonstrated strong performance on many tasks. However, several challenges and limitations to the DQN that have been identified in the literature:\n",
        "\n",
        "1. Sample complexity - DQN can require a large number of samples to learn effectively, especially in environments with high-dimensional state spaces or a large number of possible actions\n",
        "2. Convergence - DQN is known to converge to the optimal solution under certain conditions, but the convergence properties of the algorithm are not well understood and it is not guaranteed to converge in all cases\n",
        "3. Overestimation - DQN is known to sometimes overestimate the Q-values of certain actions, which can lead to suboptimal behavior\n",
        "4. Sensitivity to hyperparameters - DQN can be sensitive to the choice of hyperparameters, such as the learning rate, the discount factor, and the exploration scheme.\n",
        "\n",
        "Last year the task was to implement the rainbow algorithm [(Hessel 2017)](https://arxiv.org/pdf/1710.02298.pdf). The algorithm is a combination of several techniques for improving the performance of the DQN algorithm, which was originally proposed by DeepMind. Rainbow algorithm is able to improve the sample efficiency, stability and  performance of the DQN algorithm. The improvements include u.a.:\n",
        "\n",
        "1. Double DQN\n",
        "2. N-step Q-value estimation\n",
        "3. Noisy Layer exploration\n",
        "4. Dueling DQN\n",
        "5. Prioritized experience replay\n",
        "\n",
        "Reinforcement Learning is still a new branch of research and paradigms tend to raise and fall quite frequently. In this case, the recent literature proved that the last 3 improvementx of rainbow DQN are not as good as advertised. In their place [(Schwarzer et al. 2023)](https://arxiv.org/pdf/2305.19452.pdf) introduced other improvements that add up to the new Bigger, Better, Faster (BBF) algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homework description (relevant papers):"
      ],
      "metadata": {
        "id": "cXbb4PSlWaKc"
      },
      "id": "cXbb4PSlWaKc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recent literature proved that the last 3 improvements of rainbow DQN are not as good as advertised. In their place [(Schwarzer et al. 2023)](https://arxiv.org/pdf/2305.19452.pdf) introduced other improvements that add up to the new Bigger, Better, Faster (BBF) algorithm.\n",
        "\n",
        "In this homework, you will augment a baseline DQN implementation with components of BBF except for distributional Q-learning. To test our implementations, we will use the Lunar Lander environment with a budget of 40000 enironment steps and 30000 Q-network weight updates. You will also implement the evaluation as in [(Agarwal et al. 2022)](https://arxiv.org/pdf/2108.13264.pdf)."
      ],
      "metadata": {
        "id": "fH5KZrqpWKdx"
      },
      "id": "fH5KZrqpWKdx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homework scenario and grading"
      ],
      "metadata": {
        "id": "XA9ZWf4sWphu"
      },
      "id": "XA9ZWf4sWphu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are provided with a baseline implementation of the DQN. Your job is to expand it with the following modules:\n",
        "\n",
        "1. N-step Q-value estimation with horizon annealing **2 points**\n",
        "2. Discount annealing **1 point**\n",
        "3. Q-network resets **2 points**\n",
        "4. BBF **2 points**\n",
        "5. IQM evaluation **3 points**"
      ],
      "metadata": {
        "id": "6Cz3hXzSWsXL"
      },
      "id": "6Cz3hXzSWsXL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports, settings and helper functions:"
      ],
      "metadata": {
        "id": "RyTlYpvUApFp"
      },
      "id": "RyTlYpvUApFp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intalls:"
      ],
      "metadata": {
        "id": "FWNjncCBVR-b"
      },
      "id": "FWNjncCBVR-b"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Z_2HKKYIdygC",
      "metadata": {
        "id": "Z_2HKKYIdygC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29004a90-da21-4227-ce61-64bf9f1c2856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.2.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.0\n",
            "Collecting typeguard==2.13.3\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard\n",
            "Successfully installed typeguard-2.13.3\n",
            "Collecting torchtyping\n",
            "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtyping) (2.1.0+cu121)\n",
            "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from torchtyping) (2.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->torchtyping) (1.3.0)\n",
            "Installing collected packages: torchtyping\n",
            "Successfully installed torchtyping-0.1.4\n"
          ]
        }
      ],
      "source": [
        "! pip install swig\n",
        "! pip install gymnasium[box2d]>=0.29.0\n",
        "! pip install typeguard==2.13.3\n",
        "! pip install torchtyping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports:"
      ],
      "metadata": {
        "id": "0ipXEVlxVYUH"
      },
      "id": "0ipXEVlxVYUH"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d0728d82",
      "metadata": {
        "id": "d0728d82"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import deque\n",
        "from dataclasses import dataclass, field\n",
        "from functools import cached_property\n",
        "from typing import Any, Callable\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtyping import TensorType"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "094e4c1f",
      "metadata": {
        "id": "094e4c1f"
      },
      "source": [
        "### Hyperparameters class:\n",
        "You are given a simple class for holding the hyperparameters (do not change those!) and a helper functions for setting seeds and orthogonal weight initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e1883162",
      "metadata": {
        "id": "e1883162"
      },
      "outputs": [],
      "source": [
        "# do NOT change!\n",
        "@dataclass(frozen=True)\n",
        "class Hyperparameters:\n",
        "    capacity: int = 10000\n",
        "    init_steps: int = 10000\n",
        "    total_timesteps = 40000\n",
        "    batch_size: int = 128\n",
        "    hidden_dim: int = 128\n",
        "    optimizer_params: dict[str, Any] = field(\n",
        "        default_factory=lambda: {\n",
        "            \"lr\": 7e-4,\n",
        "            \"eps\": 1e-5,\n",
        "            \"weight_decay\": 1e-3,\n",
        "        }\n",
        "    )\n",
        "    samples: int = 3\n",
        "    target_update_freq: int = 50\n",
        "    evaluate_freq: int = 1000\n",
        "    evaluate_samples: int = 5\n",
        "\n",
        "    anneal_steps: int = 30000\n",
        "\n",
        "    init_discount: float = 0.8\n",
        "    final_discount: float = 0.99\n",
        "\n",
        "    init_epsilon: float = 0.1\n",
        "    final_epsilon: float = 0.05\n",
        "\n",
        "    init_nstep: int = 10\n",
        "    final_nstep: int = 3\n",
        "    anneal_nstep_freq: int = 2000\n",
        "\n",
        "    reset_freq: int = 30100\n",
        "    replay_ratio: int = 2\n",
        "\n",
        "    gym_id: str = \"LunarLander-v2\"\n",
        "    cuda: bool = True\n",
        "\n",
        "    @cached_property\n",
        "    def state_dim(self) -> int:\n",
        "        env = gym.make(self.gym_id)\n",
        "        return env.observation_space.shape[0]\n",
        "\n",
        "    @cached_property\n",
        "    def action_dim(self) -> int:\n",
        "        env = gym.make(self.gym_id)\n",
        "        return env.action_space.n\n",
        "\n",
        "    @cached_property\n",
        "    def device(self) -> torch.device:\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() and self.cuda else \"cpu\")\n",
        "\n",
        "\n",
        "hyperparameters = Hyperparameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions:"
      ],
      "metadata": {
        "id": "DcK6s4PlVprb"
      },
      "id": "DcK6s4PlVprb"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "97603d29",
      "metadata": {
        "id": "97603d29"
      },
      "outputs": [],
      "source": [
        "def set_seed_everywhere(\n",
        "    env: gym.wrappers.time_limit.TimeLimit,\n",
        "    seed: int,\n",
        ") -> None:\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "\n",
        "def weight_init(model: nn.Module) -> None:\n",
        "    if isinstance(model, nn.Linear):\n",
        "        nn.init.orthogonal_(model.weight.data)\n",
        "        model.bias.data.fill_(0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32176d86",
      "metadata": {
        "id": "32176d86"
      },
      "source": [
        "# 0. Baseline DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description:"
      ],
      "metadata": {
        "id": "v18hsk1lYx69"
      },
      "id": "v18hsk1lYx69"
    },
    {
      "cell_type": "markdown",
      "id": "8ed14f18",
      "metadata": {
        "id": "8ed14f18"
      },
      "source": [
        "Deep Q-Network (DQN) [(Mnih 2014)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) is a reinforcement learning algorithm that uses a deep neural network to learn a Q-function, which is a function that estimates the expected return for taking a given action in a given state. The goal of the DQN algorithm is to learn a policy that maximizes the expected return by learning the Q-function and selecting the action with the highest estimated return in each state.\n",
        "\n",
        "The DQN algorithm consists of two main components: a Q-network and an experience buffer. The Q-network is a deep neural network that takes in a state as input and outputs the estimated Q-values for each possible action. The experience buffer is a data structure that stores a set of experiences. The DQN algorithm works by interacting with the environment and storing the experiences in the experience buffer. The Q-network is then trained using a mini-batch of experiences uniformly sampled from the experience buffer. This process is known as experience replay and is used to decorrelate the experiences and to stabilize the learning process. The Q-network is updated using the loss function:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\theta} = \\frac{1}{B} \\sum_{i=1}^{B} \\bigl( \\mathrm{TD}~(s_i, a_i, s^{'}_{i}) \\bigr)^{2}\n",
        "$$\n",
        "\n",
        "With:\n",
        "\n",
        "$$\n",
        "\\mathrm{TD}~(s_i, a_i, s^{'}_{i}) = Q_{\\theta}~(s_i,a_i) - \\bigl(r_{(s_i,a_i,s_{i}^{'})} + \\gamma ~ \\underset{a^{'}_{i} \\sim \\bar{Q}_{\\theta}}{\\mathrm{max}} ~ \\bar{Q}_{\\theta}~(s_{i}^{'},a_{i}^{'}) \\bigr)\n",
        "$$\n",
        "\n",
        "Where $Q_{\\theta}$ and $\\bar{Q}_{\\theta}$ denote learned and target Q-networks respectively. The target network is a copy of the Q-network that is updated less frequently, and using it to compute the target Q-values helps to stabilize the learning process and improve the performance of the DQN algorithm. Note that to increase stability of training we use Huber loss (smooth_l1_loss) instead of L2.\n",
        "\n",
        "There are several ways to incorporate exploration into the DQN algorithm. One common method is to use an $\\epsilon$-greedy exploration strategy, where the agent takes a random action with probability $\\epsilon$ and takes the action with the highest estimated Q-value with probability $1 - \\epsilon$. The value of $\\epsilon$ is typically decreased over time, so that the agent initially explores more and then gradually shifts towards exploitation as it learns more about the environment.\n",
        "\n",
        "Below, you will find the implementaiton of all the components of a basic DQN:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ExperienceBuffer:"
      ],
      "metadata": {
        "id": "ePfkDHmIYpO_"
      },
      "id": "ePfkDHmIYpO_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experience buffer** - a data structure that stores a set of transitions, where a transition is typically represented as a tuple $(s, a, r, s', t)$, where $s$ is the state, $a$ is the action taken in state $s$, $r$ is the reward received by performing $a$ in $s$ and getting to $s'$, $s'$ is the new state observed after performing $a$ in $s$ and $t$ is the termination boolean (true if $s'$ is terminal). The **ExperienceBuffer** class below is using NumPy arrays has two methods:\n",
        "\n",
        "1. *add* - adds transition to the buffer\n",
        "2. *sample* - samples a batch of transitions from the buffer"
      ],
      "metadata": {
        "id": "sguGxrlrYuTs"
      },
      "id": "sguGxrlrYuTs"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "53e147c5",
      "metadata": {
        "id": "53e147c5"
      },
      "outputs": [],
      "source": [
        "class ExperienceBuffer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        hyperparameters: Hyperparameters,\n",
        "    ) -> None:\n",
        "        self.states = np.zeros((hyperparameters.capacity, hyperparameters.state_dim), dtype=np.float32)\n",
        "        self.actions = np.zeros((hyperparameters.capacity, 1), dtype=np.int64)\n",
        "        self.rewards = np.zeros((hyperparameters.capacity, 1), dtype=np.float32)\n",
        "        self.next_states = np.zeros((hyperparameters.capacity, hyperparameters.state_dim), dtype=np.float32)\n",
        "        self.terminals = np.zeros((hyperparameters.capacity, 1), dtype=np.int64)\n",
        "        self.full = False\n",
        "        self.idx = 0\n",
        "        self.hyperparameters = hyperparameters\n",
        "\n",
        "    def add(\n",
        "        self,\n",
        "        state: np.ndarray,\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        next_state: np.ndarray,\n",
        "        terminal: bool,\n",
        "    ) -> None:\n",
        "        self.states[self.idx, :] = state\n",
        "        self.actions[self.idx, :] = action\n",
        "        self.rewards[self.idx, :] = reward\n",
        "        self.next_states[self.idx, :] = next_state\n",
        "        self.terminals[self.idx, :] = 1 if terminal else 0\n",
        "        self.idx += 1\n",
        "        if self.idx == self.hyperparameters.capacity:\n",
        "            self.full = True\n",
        "            self.idx = 0\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "    ) -> tuple[\n",
        "        TensorType[\"batch\", \"state_dim\"],\n",
        "        TensorType[\"batch\", 1],\n",
        "        TensorType[\"batch\", 1],\n",
        "        TensorType[\"batch\", \"state_dim\"],\n",
        "        TensorType[\"batch\", 1],\n",
        "    ]:\n",
        "        idx = (\n",
        "            np.random.permutation(self.hyperparameters.capacity)[: self.hyperparameters.batch_size]\n",
        "            if self.full\n",
        "            else np.random.permutation(self.idx - 1)[: self.hyperparameters.batch_size]\n",
        "        )\n",
        "        states = torch.from_numpy(self.states[idx]).to(self.hyperparameters.device)\n",
        "        actions = torch.from_numpy(self.actions[idx]).to(self.hyperparameters.device)\n",
        "        rewards = torch.from_numpy(self.rewards[idx]).to(self.hyperparameters.device)\n",
        "        next_states = torch.from_numpy(self.next_states[idx]).to(self.hyperparameters.device)\n",
        "        terminals = torch.from_numpy(self.terminals[idx]).long().to(self.hyperparameters.device) # why long?\n",
        "        return states, actions, rewards, next_states, terminals\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QNetwork:"
      ],
      "metadata": {
        "id": "yjBmqgLnYdDR"
      },
      "id": "yjBmqgLnYdDR"
    },
    {
      "cell_type": "markdown",
      "id": "9fdcd965",
      "metadata": {
        "id": "9fdcd965"
      },
      "source": [
        "**QNetwork** - a simple dense MLP. Note the output size being equal to the amount of actions in the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e3fb7c14",
      "metadata": {
        "id": "e3fb7c14"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            hyperparameters: Hyperparameters,\n",
        "        ) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(hyperparameters.state_dim, hyperparameters.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hyperparameters.hidden_dim, hyperparameters.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hyperparameters.hidden_dim, hyperparameters.action_dim),\n",
        "        )\n",
        "        self.apply(weight_init)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: TensorType[\"batch\", \"state_dim\"],\n",
        "        ) -> TensorType[\"batch\", \"actions_dim\"]:\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN agent:"
      ],
      "metadata": {
        "id": "6jkZ-h5nYE2B"
      },
      "id": "6jkZ-h5nYE2B"
    },
    {
      "cell_type": "markdown",
      "id": "03ed7a71",
      "metadata": {
        "id": "03ed7a71"
      },
      "source": [
        "**DQN agent** - implementation of the callbacks required to learn the DQN algorithm. The class has following methods:\n",
        "\n",
        "1. *get_action* - returns action in given state using $\\epsilon$-greedy\n",
        "2. *anneal* - reduces the value of $\\epsilon$ dependent on the training step\n",
        "3. *update* - samples a batch of transitions from the experience buffer and performs a DQN update\n",
        "4. *update_target* - performs a hard update on the target Q network $\\bar{Q}_{\\theta}$\n",
        "5. *evaluate* - performs evaluation of the agent with a greedy policy\n",
        "6. *reset* - resets the agent (used between seeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "28b71037",
      "metadata": {
        "id": "28b71037"
      },
      "outputs": [],
      "source": [
        "class DQNBaseline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        hyperparameters: Hyperparameters,\n",
        "    ) -> None:\n",
        "        self.hyperparameters = hyperparameters\n",
        "        self.buffer = ExperienceBuffer(self.hyperparameters)\n",
        "        self.q_net = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
        "        self.q_target = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
        "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), **self.hyperparameters.optimizer_params)\n",
        "\n",
        "        self.epsilon = self.hyperparameters.init_epsilon\n",
        "        self.discount = self.hyperparameters.final_discount # todo that would need to change if discount annealing implemented\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.buffer = ExperienceBuffer(self.hyperparameters)\n",
        "        self.epsilon = self.hyperparameters.init_epsilon\n",
        "        self.q_net = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
        "        self.q_target = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
        "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), **self.hyperparameters.optimizer_params)\n",
        "\n",
        "    def get_action(\n",
        "        self,\n",
        "        state: np.ndarray,\n",
        "        exploration: bool = True,\n",
        "    ) -> int:\n",
        "        with torch.no_grad():\n",
        "            return (\n",
        "                np.random.randint(self.hyperparameters.action_dim)\n",
        "                if np.random.sample() < self.epsilon and exploration\n",
        "                else torch.argmax(self.q_net(state)).item()\n",
        "            )\n",
        "\n",
        "    def anneal(\n",
        "        self,\n",
        "        step: int,\n",
        "    ) -> None:\n",
        "        self.epsilon = (\n",
        "            ((self.hyperparameters.final_epsilon - self.hyperparameters.init_epsilon) / self.hyperparameters.anneal_steps) * step\n",
        "            + self.hyperparameters.init_epsilon\n",
        "            if step < self.hyperparameters.anneal_steps # should be <=\n",
        "            else self.epsilon\n",
        "        )\n",
        "\n",
        "    def update(self) -> None:\n",
        "        states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
        "        with torch.no_grad():\n",
        "            q_ns = torch.max(self.q_target(next_states), dim=1)[0].unsqueeze(1) # (torch.max returns (max, max_indices), unsqueeze makes it's shape (batch,1))\n",
        "        q_targets = rewards + (1 - terminals) * self.discount * q_ns\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        q_values = self.q_net(states).gather(1, actions) #(batch, actions_dim) -> (batch, 1) # for each sample chooses the appropriate value corresponding to the action taken in that sample\n",
        "        loss = nn.functional.smooth_l1_loss(q_values, q_targets) #deafault beta is 1 and default reduction is mean\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target(self) -> None:\n",
        "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        samples: int,\n",
        "    ) -> float:\n",
        "        with torch.no_grad():\n",
        "            env_test = gym.make(self.hyperparameters.gym_id, max_episode_steps=1000)\n",
        "            eval_rewards = np.zeros((samples,))\n",
        "            for i in range(samples):\n",
        "                state, _ = env_test.reset()\n",
        "                episode_reward = 0\n",
        "                while True:\n",
        "                    action = self.get_action(torch.tensor(state).unsqueeze(0).to(self.hyperparameters.device), False)\n",
        "                    next_state, reward, terminal, truncated, _ = env_test.step(action)\n",
        "                    episode_reward += reward\n",
        "                    state = next_state\n",
        "                    if terminal or truncated:\n",
        "                        # eval_reward += episode_reward / samples\n",
        "                        eval_rewards[i] = episode_reward\n",
        "                        break\n",
        "        return eval_rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f539a644",
      "metadata": {
        "id": "f539a644"
      },
      "source": [
        "Finally, you are presented with the training loop for the DQN agents (do NOT change this):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN agent training:"
      ],
      "metadata": {
        "id": "4pzgOnB3YLr_"
      },
      "id": "4pzgOnB3YLr_"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "385a9330",
      "metadata": {
        "id": "385a9330"
      },
      "outputs": [],
      "source": [
        "def train_agent(\n",
        "    hyperparameters: Hyperparameters,\n",
        "    agent: DQNBaseline,\n",
        ") -> np.ndarray:\n",
        "    results = np.zeros(\n",
        "        (\n",
        "            hyperparameters.total_timesteps // hyperparameters.evaluate_freq,\n",
        "            hyperparameters.samples,\n",
        "            hyperparameters.evaluate_samples\n",
        "        )\n",
        "    )\n",
        "    for seed_idx, seed in enumerate(range(hyperparameters.samples)): # seed = seed_idx\n",
        "        env = gym.make(hyperparameters.gym_id, max_episode_steps=1000)\n",
        "        agent.reset()\n",
        "        set_seed_everywhere(env, seed)\n",
        "        state, _ = env.reset()\n",
        "        for step in range(hyperparameters.total_timesteps):\n",
        "            if step == hyperparameters.init_steps:\n",
        "                start_time = time.time()\n",
        "            action = agent.get_action(\n",
        "                torch.tensor(state).unsqueeze(0).to(hyperparameters.device)\n",
        "            )\n",
        "            next_state, reward, terminal, truncated, _ = env.step(action)\n",
        "            agent.buffer.add(state, action, reward, next_state, terminal or truncated)\n",
        "            agent.anneal(step)\n",
        "            state = next_state\n",
        "            if step >= hyperparameters.init_steps:\n",
        "                for update_num in range(hyperparameters.replay_ratio):\n",
        "                    agent.update()\n",
        "                    if (\n",
        "                        step * hyperparameters.replay_ratio + update_num + 1\n",
        "                    ) % hyperparameters.target_update_freq == 0:\n",
        "                        agent.update_target()\n",
        "                if (step + 1) % hyperparameters.evaluate_freq == 0:\n",
        "                    eval_rewards = agent.evaluate(hyperparameters.evaluate_samples)\n",
        "                    results[step // hyperparameters.evaluate_freq, seed] = eval_rewards\n",
        "                    print(\n",
        "                        \"\\rSample number: {} Step: {} Evaluation reward: {:.2f} Samples per second: {:}\".format(\n",
        "                            seed_idx + 1,\n",
        "                            step,\n",
        "                            eval_rewards.mean(),\n",
        "                            int(\n",
        "                                (step - hyperparameters.init_steps)\n",
        "                                / (time.time() - start_time)\n",
        "                            ),\n",
        "                        ),\n",
        "                        end=\"\",\n",
        "                    )\n",
        "            if terminal or truncated: # why not terminal or truncated?\n",
        "                state, _ = env.reset()\n",
        "                episode_reward = 0 # what for?\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff186311",
      "metadata": {
        "id": "ff186311"
      },
      "source": [
        "The training of the baseline DQN agent is implemented in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "938d29b1",
      "metadata": {
        "id": "938d29b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eadd22ce-5f10-4ea7-8d84-3bdeb8787bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample number: 3 Step: 39999 Evaluation reward: 208.87 Samples per second: 35"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3474662460827616"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "results_dict = {}\n",
        "agent = DQNBaseline(hyperparameters) #hyperparameters is global variable - instant of Hyperparameters\n",
        "results_dqn = train_agent(hyperparameters, agent)\n",
        "results_dict[\"DQNBaseline\"] = results_dqn\n",
        "results_dqn.mean(1)[10:].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3376e3e2",
      "metadata": {
        "id": "3376e3e2"
      },
      "source": [
        "# Below the proper task begins:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. N-step Q-value estimation with horizon annealing"
      ],
      "metadata": {
        "id": "KzlYu9rSX5De"
      },
      "id": "KzlYu9rSX5De"
    },
    {
      "cell_type": "markdown",
      "id": "004bf3a3",
      "metadata": {
        "id": "004bf3a3"
      },
      "source": [
        "$N$-step TD ($\\mathrm{TD}_{n}$) was introduced long before neural network based RL. In regular TD, we supervise the Q-network with single-step reward summed with highest Q-value of the next state. In contrast to that, $\\mathrm{TD}_{n}$ accumulated rewards over $n$ steps and sums it with the highest Q-value of the state that occured after $n$ steps [(Sutton 1988)](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf). Double DQN $\\mathrm{TD}_{n}$ loss is defined by:\n",
        "\n",
        "$$\n",
        "\\mathrm{TD}_{n}(s_i, a_i, s^{'}_{i+n}) = Q_{\\theta}~(s_i,a_i) - \\biggl(\\sum_{k=0}^{n-1} \\gamma^{k} ~ r_{(s_{i+k},a_{i+k},s_{i+k}^{'})} + \\gamma^{n} \\underset{a^{'}_{i+n} \\sim \\bar{Q}_{\\theta}}{\\mathrm{max}} ~ \\bar{Q}_{\\theta}~(s_{i+n}^{'},a_{i+n}^{'}) \\biggr)\n",
        "$$\n",
        "\n",
        "The horizon hyperparameter (n) is going to be annealed from the `hyperparameters.init_nstep` to `hyperparameters.final_nstep`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Implement NStepExperienceBuffer"
      ],
      "metadata": {
        "id": "IT8X6Cc0XynW"
      },
      "id": "IT8X6Cc0XynW"
    },
    {
      "cell_type": "markdown",
      "id": "f448764d",
      "metadata": {
        "id": "f448764d"
      },
      "source": [
        "Implementing $\\mathrm{TD}_{n}$ requires changes to the ExperienceBuffer class. We will implement those changes using the **deque** module. This module will store $n$ of the most recent transitions, and will act as a middleware between agent and buffers main storage. As compared to single step reward and $s_{i}^{'}$ stored by the simple ExperienceBuffer, the main storage of this upgraded buffer should store $n$ step rewards and $s_{i+n}^{'}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "12560df7",
      "metadata": {
        "id": "12560df7"
      },
      "outputs": [],
      "source": [
        "class NStepExperienceBuffer(ExperienceBuffer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            hyperparameters: Hyperparameters,\n",
        "        ) -> None:\n",
        "        super().__init__(hyperparameters)\n",
        "        self.memories = deque(maxlen=self.hyperparameters.init_nstep)\n",
        "        self.nstep = self.hyperparameters.init_nstep\n",
        "        self.discount = self.hyperparameters.final_discount\n",
        "\n",
        "\n",
        "    def set_nstep(\n",
        "            self,\n",
        "            value: int\n",
        "        ) -> None:\n",
        "        ############### TODO ###############\n",
        "        if value != self.nstep:\n",
        "            self.nstep = value\n",
        "            self.memories = deque(maxlen=value)\n",
        "            #I should 'empty' the buffer bc as the nstep changes\n",
        "            #so will the power to which the discount needs to be raised\n",
        "            #when calculating the loss\n",
        "            #but granted the change is slow entries in the buffer don't become\n",
        "            #outdated too much before being replaced by the new ones\n",
        "\n",
        "            #Should I want to zero the buffer, here is the code:\n",
        "            hyperparameters = self.hyperparameters\n",
        "            self.states = np.zeros((hyperparameters.capacity, hyperparameters.state_dim), dtype=np.float32)\n",
        "            self.actions = np.zeros((hyperparameters.capacity, 1), dtype=np.int64)\n",
        "            self.rewards = np.zeros((hyperparameters.capacity, 1), dtype=np.float32)\n",
        "            self.next_states = np.zeros((hyperparameters.capacity, hyperparameters.state_dim), dtype=np.float32)\n",
        "            self.terminals = np.zeros((hyperparameters.capacity, 1), dtype=np.int64)\n",
        "            self.full = False\n",
        "            self.idx = 0\n",
        "\n",
        "        ####################################\n",
        "\n",
        "\n",
        "    def get_nstep(self) -> tuple[np.ndarray, int, float, np.ndarray, bool]:\n",
        "        ############### TODO ###############\n",
        "        #I assume that in the first n_step steps of the geme there would be no terminal\n",
        "        # then when the terrminal appears I sclear the memories in order to care for the internal state of the structures\n",
        "        #  unless nstep = 1 some states near the very end won't be learned but nstep is small so that is not a problem\n",
        "        if len(self.memories) == 0:\n",
        "            return None\n",
        "        state = self.memories[0][0]\n",
        "        action = self.memories[0][1]\n",
        "        next_state = self.memories[-1][3]\n",
        "        terminal = self.memories[-1][4]\n",
        "        sum = 0\n",
        "        current_discount = 1\n",
        "        for e in self.memories:\n",
        "            _, _, reward_from_step, _, _ = e\n",
        "            sum += reward_from_step * current_discount\n",
        "            current_discount *= self.discount\n",
        "        if terminal:\n",
        "            self.memories.clear()\n",
        "        reward = sum\n",
        "        ####################################\n",
        "        return state, action, reward, next_state, terminal\n",
        "\n",
        "\n",
        "    def add(\n",
        "        self,\n",
        "        state: np.ndarray,\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        next_state: np.ndarray,\n",
        "        terminal: bool,\n",
        "    ) -> None:\n",
        "        memory = (state, action, reward, next_state, terminal)\n",
        "        self.memories.append(memory)\n",
        "        if len(self.memories) >= self.nstep:\n",
        "            ############### TODO ###############\n",
        "            (state, action, reward, next_state, terminal) = self.get_nstep()\n",
        "            self.states[self.idx, :] = state\n",
        "            self.actions[self.idx, :] = action\n",
        "            self.rewards[self.idx, :] = reward\n",
        "            self.next_states[self.idx, :] = next_state\n",
        "            self.terminals[self.idx, :] = 1 if terminal else 0\n",
        "            ####################################\n",
        "            self.idx += 1\n",
        "            if self.idx == self.hyperparameters.capacity:\n",
        "                self.full = True\n",
        "                self.idx = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2: Implement and train N-step annealing DQN"
      ],
      "metadata": {
        "id": "4S2woX-EXuBX"
      },
      "id": "4S2woX-EXuBX"
    },
    {
      "cell_type": "markdown",
      "id": "34d45601",
      "metadata": {
        "id": "34d45601"
      },
      "source": [
        "Implement **NStepAnnealing** by including the annealing step. It should decrease from the **hyperparameters.init_nstep** value to the **hyperparameters.final_nstep** value by 1 every **hyperparameters.anneal_nstep_freq** steps.\n",
        "\n",
        "Remember to properly set `nstep` parameter in all relevant attributes of the **NStepAnnealing** class!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "0d1cb4c6",
      "metadata": {
        "id": "0d1cb4c6"
      },
      "outputs": [],
      "source": [
        "class NStepAnnealing(DQNBaseline):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hyperparameters: Hyperparameters,\n",
        "    ) -> None:\n",
        "        super().__init__(hyperparameters)\n",
        "        self.nstep = self.hyperparameters.init_nstep\n",
        "        self.buffer = NStepExperienceBuffer(hyperparameters)\n",
        "        self.last_discount = self.discount ** self.nstep #my addition\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        super().reset()\n",
        "        ############### TODO ###############\n",
        "        self.nstep = self.hyperparameters.init_nstep\n",
        "        self.buffer = NStepExperienceBuffer(self.hyperparameters)\n",
        "        self.last_discount = self.discount ** self.nstep\n",
        "        ####################################\n",
        "\n",
        "    def set_nstep(\n",
        "            self,\n",
        "            value: int,\n",
        "        ) -> None:\n",
        "        ############### TODO ###############\n",
        "        self.nstep = value\n",
        "        self.buffer.set_nstep(self.nstep)\n",
        "        self.last_discount = self.discount ** self.nstep\n",
        "        ####################################\n",
        "\n",
        "    def anneal(\n",
        "        self,\n",
        "        step: int,\n",
        "    ) -> None:\n",
        "        super().anneal(step=step)\n",
        "        ############### TODO ###############\n",
        "        if ((step % self.hyperparameters.anneal_nstep_freq) == 0):\n",
        "            difference = (step // self.hyperparameters.anneal_nstep_freq)\n",
        "            if (difference > 0) and (difference <= (self.hyperparameters.final_nstep - self.hyperparameters.init_nstep)):\n",
        "                self.set_nstep(self.hyperparameters.init_nstep - difference)\n",
        "        ####################################\n",
        "\n",
        "    def update(self) -> None:\n",
        "        ############### TODO ###############\n",
        "        states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
        "        with torch.no_grad():\n",
        "            q_ns = torch.max(self.q_target(next_states), dim=1)[0].unsqueeze(1)\n",
        "        q_targets = rewards + (1 - terminals) * self.last_discount * q_ns\n",
        "        ####################################\n",
        "        self.optimizer.zero_grad()\n",
        "        q_values = self.q_net(states).gather(1, actions)\n",
        "        loss = nn.functional.smooth_l1_loss(q_values, q_targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a49e8b4",
      "metadata": {
        "id": "1a49e8b4"
      },
      "source": [
        "Launch the training of the NStepAnnealing DQN agent and observe difference in results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27e7900",
      "metadata": {
        "id": "e27e7900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71045da-62be-4950-e969-74d49d1f82ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample number: 1 Step: 28999 Evaluation reward: -74.84 Samples per second: 31"
          ]
        }
      ],
      "source": [
        "agent = NStepAnnealing(hyperparameters)\n",
        "results_dqn2 = train_agent(hyperparameters, agent)\n",
        "results_dict[\"NStepAnnealing\"] = results_dqn2\n",
        "results_dqn2.mean(1)[-10:].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Discount annealing"
      ],
      "metadata": {
        "id": "HZM9HMswXot4"
      },
      "id": "HZM9HMswXot4"
    },
    {
      "cell_type": "markdown",
      "id": "YtM0SoH2c2hb",
      "metadata": {
        "id": "YtM0SoH2c2hb"
      },
      "source": [
        "Remember that the loss in the baseline DQN is defined as:\n",
        "\n",
        "$$\n",
        "\\mathrm{TD}~(s_i, a_i, s^{'}_{i}) = Q_{\\theta}~(s_i,a_i) - \\bigl(r_{(s_i,a_i,s_{i}^{'})} + \\gamma ~ \\underset{a^{'}_{i} \\sim \\bar{Q}_{\\theta}}{\\mathrm{max}} ~ \\bar{Q}_{\\theta}~(s_{i}^{'},a_{i}^{'}) \\bigr)\n",
        "$$\n",
        "\n",
        "This section includes implmenetation of annealing of the discount $\\gamma$ hyperperameter. It should lineary decrease from the **hyperparameters.init_discount** value to the **hyperparameters.final_discount** value over **anneal_steps** steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "Yb19-g1DaCl_",
      "metadata": {
        "id": "Yb19-g1DaCl_"
      },
      "outputs": [],
      "source": [
        "class DiscountAnnealing(DQNBaseline):\n",
        "    def __init__(\n",
        "            self,\n",
        "            hyperparameters: Hyperparameters,\n",
        "        ) -> None:\n",
        "        super().__init__(hyperparameters)\n",
        "        self.discount = self.hyperparameters.init_discount\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        super().reset()\n",
        "        ############### TODO ###############\n",
        "        self.discount = self.hyperparameters.init_discount\n",
        "        ####################################\n",
        "\n",
        "    def set_discount(\n",
        "            self,\n",
        "            value: int,\n",
        "        ) -> None:\n",
        "        ############### TODO ###############\n",
        "        #if value != self.discount:\n",
        "            #self.buffer = ExperienceBuffer(self.hyperparameters) #old entries\n",
        "            #in the buffer should go as they are outdated - now the discount factor\n",
        "            #is different, so the coefficients by which sequence elements are multiplied in the sum change\n",
        "            #but granted the change is slow entries in the buffer don't become\n",
        "            #outdated too much before being replaced by the new ones\n",
        "        self.discount = value\n",
        "        ####################################\n",
        "\n",
        "    def anneal(\n",
        "        self,\n",
        "        step: int,\n",
        "    ) -> None:\n",
        "        super().anneal(step=step)\n",
        "        ############### TODO ###############\n",
        "        current_discount = (\n",
        "            ((self.hyperparameters.final_discount - self.hyperparameters.init_discount) / self.hyperparameters.anneal_steps) * step\n",
        "            + self.hyperparameters.init_discount\n",
        "            if step < self.hyperparameters.anneal_steps\n",
        "            else self.hyperparameters.final_discount)\n",
        "\n",
        "        self.set_discount(current_discount)\n",
        "        ####################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "385b0699",
      "metadata": {
        "id": "385b0699"
      },
      "source": [
        "Launch the training of the DiscountAnnealing DQN agent and observe difference in results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cVZGXODmhurZ",
      "metadata": {
        "id": "cVZGXODmhurZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45536177-878f-483f-8199-1a22dd9163e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample number: 3 Step: 39999 Evaluation reward: -51.84 Samples per second: 27"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.012222345033721"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "agent = DiscountAnnealing(hyperparameters)\n",
        "results_dqn3 = train_agent(hyperparameters, agent)\n",
        "results_dict[\"DiscountAnnealing\"] = results_dqn3\n",
        "results_dqn3.mean(1)[-10:].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66747342",
      "metadata": {
        "id": "66747342"
      },
      "source": [
        "## 3. Q-network resets\n",
        "The q-networks tend to overfit to initial, low quality data and loose plasticity over time. To overcome this problem reinitialize q-networks every `self.reset_freq` updates. Remeber to reset the optimizer parameters too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bd35f21d",
      "metadata": {
        "id": "bd35f21d"
      },
      "outputs": [],
      "source": [
        "class Resets(DQNBaseline):\n",
        "    def __init__(\n",
        "            self,\n",
        "            hyperparameters: Hyperparameters,\n",
        "        ) -> None:\n",
        "        super().__init__(hyperparameters)\n",
        "        self.reset_freq = hyperparameters.reset_freq\n",
        "        ############### TODO ###############\n",
        "        self.num_updates = 0\n",
        "        ####################################\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        super().reset()\n",
        "        ############### TODO ###############\n",
        "        self.reset_freq = hyperparameters.reset_freq #though it doesn't change for now\n",
        "        self.num_updates = 0\n",
        "        ####################################\n",
        "\n",
        "    def update(self) -> None:\n",
        "        states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
        "        with torch.no_grad():\n",
        "            q_ns = torch.max(self.q_target(next_states), dim=1)[0].unsqueeze(1)\n",
        "        q_targets = rewards + (1 - terminals) * self.discount * q_ns\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        q_values = self.q_net(states).gather(1, actions)\n",
        "        loss = nn.functional.smooth_l1_loss(q_values, q_targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.num_updates += 1\n",
        "\n",
        "        ############### TODO ###############\n",
        "        if (self.num_updates % self.reset_freq == 0):\n",
        "            self.q_net = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
        "            self.q_target = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
        "            self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "            self.optimizer = optim.Adam(self.q_net.parameters(), **self.hyperparameters.optimizer_params)\n",
        "        ####################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eea3aa2",
      "metadata": {
        "id": "7eea3aa2"
      },
      "source": [
        "Launch the training of the Resets DQN agent and observe difference in results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "66fe46c0",
      "metadata": {
        "id": "66fe46c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba2dc980-df24-407f-a3c9-830bf682a3f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample number: 3 Step: 39999 Evaluation reward: -0.08 Samples per second: 44"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-14.55003192107412"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "agent = Resets(hyperparameters)\n",
        "results_dqn4 = train_agent(hyperparameters, agent)\n",
        "results_dict[\"Resets\"] = results_dqn4\n",
        "results_dqn4.mean(1)[-10:].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bd7a09d",
      "metadata": {
        "id": "3bd7a09d"
      },
      "source": [
        "## 4. BBF\n",
        "\n",
        "In this section your task is to combine all the above ideas into a single DQN agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ff5b1a1d",
      "metadata": {
        "id": "ff5b1a1d"
      },
      "outputs": [],
      "source": [
        "class BBF(DQNBaseline):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hyperparameters: Hyperparameters,\n",
        "    ) -> None:\n",
        "        super().__init__(hyperparameters)\n",
        "        self.nstep = self.hyperparameters.init_nstep\n",
        "        self.buffer = NStepExperienceBuffer(hyperparameters)\n",
        "        self.discount = self.hyperparameters.init_discount\n",
        "        self.reset_freq = hyperparameters.reset_freq\n",
        "        self.replay_ratio = hyperparameters.replay_ratio\n",
        "        self.num_updates = 0\n",
        "        #self.last_discount = self.discount ** self.nstep #not useful as self.discount changes constantly\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        super().reset()\n",
        "        self.buffer = NStepExperienceBuffer(self.hyperparameters)\n",
        "        self.nstep = self.hyperparameters.init_nstep\n",
        "        self.discount = self.hyperparameters.init_discount\n",
        "        self.num_updates = 0\n",
        "\n",
        "    def set_nstep(self, value: int) -> None:\n",
        "        ############### TODO ###############\n",
        "        self.nstep = value\n",
        "        self.buffer.set_nstep(self.nstep)\n",
        "        ####################################\n",
        "\n",
        "    def set_discount(self, value: int) -> None:\n",
        "        ############### TODO ###############\n",
        "        self.discount = value\n",
        "        self.buffer.discount = self.discount\n",
        "        ####################################\n",
        "\n",
        "    def anneal(\n",
        "        self,\n",
        "        step: int,\n",
        "    ) -> None:\n",
        "        super().anneal(step=step)\n",
        "        # N-step\n",
        "        value = max(\n",
        "            self.hyperparameters.final_nstep,\n",
        "            min(\n",
        "                self.hyperparameters.init_nstep\n",
        "                - (step - self.hyperparameters.init_steps)\n",
        "                // self.hyperparameters.anneal_nstep_freq,\n",
        "                self.hyperparameters.init_steps,\n",
        "            ),\n",
        "        )  #why self.hyperparameters.init_steps?\n",
        "        self.set_nstep(value=value)\n",
        "        # Discount\n",
        "        value = (\n",
        "            (\n",
        "                (\n",
        "                    self.hyperparameters.final_discount\n",
        "                    - self.hyperparameters.init_discount\n",
        "                )\n",
        "                / self.hyperparameters.anneal_steps\n",
        "            )\n",
        "            * step\n",
        "            + self.hyperparameters.init_discount\n",
        "            if step < self.hyperparameters.anneal_steps\n",
        "            else self.hyperparameters.final_discount #there was a bug here\n",
        "        )\n",
        "        self.set_discount(value=value)\n",
        "\n",
        "    def reset_params(self) -> None:\n",
        "        ############### TODO ###############\n",
        "        if (self.num_updates % self.reset_freq == 0):\n",
        "            self.q_net = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
        "            self.q_target = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
        "            self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "            self.optimizer = optim.Adam(self.q_net.parameters(), **self.hyperparameters.optimizer_params)\n",
        "        ####################################\n",
        "\n",
        "    def update(self) -> None:\n",
        "        states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
        "        with torch.no_grad():\n",
        "            q_ns = torch.max(self.q_target(next_states), dim=1)[0].unsqueeze(1)\n",
        "        q_targets = rewards + (1 - terminals) * self.discount**self.nstep * q_ns\n",
        "        self.optimizer.zero_grad()\n",
        "        q_values = self.q_net(states).gather(1, actions)\n",
        "        loss = nn.functional.smooth_l1_loss(q_values, q_targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.num_updates += 1\n",
        "\n",
        "        ############### TODO ###############\n",
        "        self.reset_params()\n",
        "        ####################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46b6cc41",
      "metadata": {
        "id": "46b6cc41"
      },
      "source": [
        "Launch the training of the BBF DQN agent and observe difference in results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "3d3d0955",
      "metadata": {
        "id": "3d3d0955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49158fc-87d1-4763-b81f-d1b521d2a8a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample number: 3 Step: 39999 Evaluation reward: 242.23 Samples per second: 37"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55.70655591919393"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "agent = BBF(hyperparameters)\n",
        "results_dqn5 = train_agent(hyperparameters, agent)\n",
        "results_dict[\"BBF\"] = results_dqn5\n",
        "results_dqn5.mean(1)[-10:].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Aggregate evaluation data and compute IQM metric"
      ],
      "metadata": {
        "id": "jteFQv1NXP-v"
      },
      "id": "jteFQv1NXP-v"
    },
    {
      "cell_type": "markdown",
      "id": "42a78ec8",
      "metadata": {
        "id": "42a78ec8"
      },
      "source": [
        "At the end we ask you to present collected data according to highest standards in the area. Presented solution were suggested in the paper [Deep Reinforcement Learning at the Edge of the\n",
        "Statistical Precipice]().\n",
        "\n",
        "To aggregate performance we will use interquartile mean (IQM) instead of average.\n",
        "\n",
        "First implement IQM as an average of middle 50% of combined runs results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "b88d1364",
      "metadata": {
        "id": "b88d1364"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "def IQM(combined_runs: np.ndarray) -> float:\n",
        "    ############### TODO ###############\n",
        "    return stats.trim_mean(combined_runs, proportiontocut = 0.25) #default: axis = 0\n",
        "    ####################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17c7e136",
      "metadata": {
        "id": "17c7e136"
      },
      "source": [
        "Second we ask you to compute boostrap confidence interval to express uncertainty in the average performance. Follow these steps for implementation:\n",
        "1. Repeat the following process `n_samples` times: generate a sample of the same size as your original data by randomly sampling from it with replacement.\n",
        "2. On each iteration, calculate specified statistic (in this case, IQM) based on the generated sample.\n",
        "3. After completing all iterations, you will have a collection of `n_samples` IQM values. To construct a confidence interval, identify two quantiles, denoted as p1 and p2. These quantiles should be equidistant from the median (50%) and the distance between them should correspond to the desired confidence level. For instance, if the confidence level is 90%, set p1=0.05 and p2=0.95.\n",
        "4. Finally, return the p1-quantile and p2-quantile of your IQM values as the lower and upper bounds of your bootstrap confidence interval.\n",
        "\n",
        "To obtain maximum points for this task you must not use python \"for\" loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "cd7f352d",
      "metadata": {
        "id": "cd7f352d"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "def bootstrap(\n",
        "    data: np.ndarray,\n",
        "    statistic: Callable,\n",
        "    n_resamples: int = 9999,\n",
        "    confidence_level: float = 0.95,\n",
        ") -> tuple[float, float]:\n",
        "    ############### TODO ###############\n",
        "    n_samples = n_resamples\n",
        "    p1 = (1 - confidence_level) / 2\n",
        "    p2 = (1 + confidence_level) / 2\n",
        "\n",
        "    #sampling:\n",
        "    data = data.reshape(-1)\n",
        "    rng = np.random.default_rng()\n",
        "    samples = rng.choice(data, size=(data.size, n_samples), replace=True)\n",
        "    iqms = statistic(samples)\n",
        "\n",
        "    #quantiles:\n",
        "    lower = stats.scoreatpercentile(iqms, per = p1 * 100)\n",
        "    upper = stats.scoreatpercentile(iqms, per = p2 * 100)\n",
        "\n",
        "    return (lower, upper)\n",
        "\n",
        "    ####################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13829bf",
      "metadata": {
        "id": "e13829bf"
      },
      "source": [
        "Third implement aggregating function. For each method compute IQM nad confidence intervals using data from the last 10 evaluations, all eval runs and all seeds.\n",
        "Return DataFrame with the following columns: \"method_name\", \"IQM\", \"confidence_lower_bound\", \"confidence_upper_bound\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "3d691049",
      "metadata": {
        "id": "3d691049"
      },
      "outputs": [],
      "source": [
        "def aggregate_data(results: dict[str, np.ndarray]) -> pd.DataFrame:\n",
        "    ############### TODO ###############\n",
        "    names = []\n",
        "    iqms = []\n",
        "    l_bounds = []\n",
        "    u_bounds = []\n",
        "    for key in results.keys():\n",
        "        names.append(key)\n",
        "        data = results[key][-10:]\n",
        "        data = data.reshape(-1)\n",
        "        iqms.append(IQM(data))\n",
        "        confidence_lower_bound, confidence_upper_bound = bootstrap(data, IQM)\n",
        "        l_bounds.append(confidence_lower_bound)\n",
        "        u_bounds.append(confidence_upper_bound)\n",
        "\n",
        "    d = {\"method_name\" : names, \"IQM\" : iqms, \"confidence_lower_bound\" : l_bounds, \"confidence_upper_bound\" : u_bounds}\n",
        "    return pd.DataFrame(data=d)\n",
        "    ####################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aggregate_data(results_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "FNvbAOK-qgZl",
        "outputId": "db69492e-6c32-4312-dce2-866006c8ec7a"
      },
      "id": "FNvbAOK-qgZl",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         method_name         IQM  confidence_lower_bound  \\\n",
              "0        DQNBaseline  114.273682               84.513310   \n",
              "1     NStepAnnealing -208.228854             -264.256521   \n",
              "2  DiscountAnnealing  -23.139699              -32.103859   \n",
              "3             Resets  -24.259599              -35.810201   \n",
              "4                BBF   50.548947               22.347331   \n",
              "\n",
              "   confidence_upper_bound  \n",
              "0              143.895971  \n",
              "1             -158.528264  \n",
              "2               -7.435977  \n",
              "3              -14.791498  \n",
              "4               79.748713  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-874c8b7d-d7dd-4d8f-a0dd-5fa5f630b452\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>method_name</th>\n",
              "      <th>IQM</th>\n",
              "      <th>confidence_lower_bound</th>\n",
              "      <th>confidence_upper_bound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DQNBaseline</td>\n",
              "      <td>114.273682</td>\n",
              "      <td>84.513310</td>\n",
              "      <td>143.895971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NStepAnnealing</td>\n",
              "      <td>-208.228854</td>\n",
              "      <td>-264.256521</td>\n",
              "      <td>-158.528264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DiscountAnnealing</td>\n",
              "      <td>-23.139699</td>\n",
              "      <td>-32.103859</td>\n",
              "      <td>-7.435977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Resets</td>\n",
              "      <td>-24.259599</td>\n",
              "      <td>-35.810201</td>\n",
              "      <td>-14.791498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BBF</td>\n",
              "      <td>50.548947</td>\n",
              "      <td>22.347331</td>\n",
              "      <td>79.748713</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-874c8b7d-d7dd-4d8f-a0dd-5fa5f630b452')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-874c8b7d-d7dd-4d8f-a0dd-5fa5f630b452 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-874c8b7d-d7dd-4d8f-a0dd-5fa5f630b452');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f47ff120-6aa8-442a-991b-abda1b60ff5e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f47ff120-6aa8-442a-991b-abda1b60ff5e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f47ff120-6aa8-442a-991b-abda1b60ff5e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Plot collected results"
      ],
      "metadata": {
        "id": "Kwu9xNVRXXsZ"
      },
      "id": "Kwu9xNVRXXsZ"
    },
    {
      "cell_type": "markdown",
      "id": "5a39b8c3",
      "metadata": {
        "id": "5a39b8c3"
      },
      "source": [
        "Use the provided function and replace the example data with your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "27373aca",
      "metadata": {
        "id": "27373aca"
      },
      "outputs": [],
      "source": [
        "example_data = pd.DataFrame(\n",
        "    {\n",
        "        \"method_name\": [\"baseline\", \"discount_annealing\", \"n_step_annealing\", \"resets\", \"combined\"],\n",
        "        \"IQM\": [-24, 20, 23, 30, 40],\n",
        "        \"confidence_lower_bound\": [-30, 14, 20, 25, 33],\n",
        "        \"confidence_upper_bound\": [-20, 22, 27, 38, 44],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "d22cf465",
      "metadata": {
        "id": "d22cf465"
      },
      "outputs": [],
      "source": [
        "def plot_results(data: pd.DataFrame) -> tuple[matplotlib.figure.Figure, matplotlib.axes._axes.Axes]:\n",
        "    assert data.shape == (5, 4)\n",
        "    assert set(data.columns) == set(\n",
        "        [\n",
        "            \"method_name\",\n",
        "            \"IQM\",\n",
        "            \"confidence_lower_bound\",\n",
        "            \"confidence_upper_bound\",\n",
        "        ]\n",
        "    )\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    colors = [\"skyblue\", \"lightgreen\", \"lightcoral\", \"orange\", \"lightpink\"]\n",
        "\n",
        "    for i, method in enumerate(data[\"method_name\"]):\n",
        "        mean = data.at[i, \"IQM\"]\n",
        "        lower_bound = data.at[i, \"confidence_lower_bound\"]\n",
        "        upper_bound = data.at[i, \"confidence_upper_bound\"]\n",
        "\n",
        "        rect_width = upper_bound - lower_bound\n",
        "\n",
        "        ax.plot(\n",
        "            [mean, mean],\n",
        "            [i - 0.4, i + 0.4],\n",
        "            color=\"black\",\n",
        "            linewidth=2,\n",
        "            label=\"Mean\" if i == 0 else \"\",\n",
        "        )\n",
        "\n",
        "        rect = plt.Rectangle(\n",
        "            (lower_bound, i - 0.4),\n",
        "            rect_width,\n",
        "            0.8,\n",
        "            color=colors[i],\n",
        "            alpha=0.7,\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    ax.set_yticks(\n",
        "        range(len(data)),\n",
        "        data[\"method_name\"],\n",
        "    )\n",
        "    ax.set_title(\"Results of each method with empirical confidence intervals\")\n",
        "\n",
        "    ax.grid(\n",
        "        axis=\"y\",\n",
        "        linestyle=\"--\",\n",
        "        alpha=0.7,\n",
        "    )\n",
        "    ax.spines[\"left\"].set_visible(False)\n",
        "    ax.spines[\"top\"].set_visible(False)\n",
        "    ax.spines[\"right\"].set_visible(False)\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "S9hu7jlrnw5f",
      "metadata": {
        "id": "S9hu7jlrnw5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "40cc937b-058d-4478-832f-bbb5bf4fd883"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig, ax = plot_results(aggregate_data(results_dict))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eWnIJ-egrDEh"
      },
      "id": "eWnIJ-egrDEh",
      "execution_count": 53,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "g6t0j59HZK9q",
        "S7Ug_DKpfnqL",
        "TYnn3zYlW6jb",
        "cXbb4PSlWaKc",
        "XA9ZWf4sWphu",
        "RyTlYpvUApFp",
        "FWNjncCBVR-b",
        "0ipXEVlxVYUH",
        "094e4c1f",
        "DcK6s4PlVprb",
        "32176d86",
        "v18hsk1lYx69",
        "ePfkDHmIYpO_",
        "yjBmqgLnYdDR",
        "6jkZ-h5nYE2B",
        "IT8X6Cc0XynW",
        "4S2woX-EXuBX",
        "HZM9HMswXot4",
        "66747342",
        "3bd7a09d",
        "jteFQv1NXP-v"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "6809863f01cf54cb8cf26991fcf8425a337722d7d35212492765d6bf47d2da35"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}