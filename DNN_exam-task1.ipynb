{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 (6p)\n",
        "Your task is to modify the custom implementation of MultiHeadAttention. This custom implementation, currently, enables each token to attent to every other token.\n",
        "\n",
        "\n",
        "Your job is to change this behavior in a specific way.\n",
        "Let $S$ be our input sequence of length $2 \\cdot k$:\n",
        "- tokens on positions $i \\lt k$ should attend to prefix of $S$ of length $k$ ($S[:k]$) - every token up to position k\n",
        "- tokens on positions $i \\ge k$ should attend to prefix of $S$  of length $i + 1$ ($S[:i + 1]$) - every previous token and itself\n",
        "\n",
        "(Note: You can assume the sequence length is always an even number)."
      ],
      "metadata": {
        "id": "VFiew9xpybfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_head):\n",
        "      super().__init__()\n",
        "      self.d_model = d_model\n",
        "      self.num_heads = num_heads\n",
        "      self.d_head = d_head\n",
        "\n",
        "      self.W_Q = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_K = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_V = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_O = torch.nn.Linear(num_heads*d_head, d_model, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      seq_len, batch_size, _ = x.shape\n",
        "\n",
        "      Q = self.W_Q(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "      K = self.W_K(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "      V = self.W_V(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "\n",
        "      scaled_QK = torch.einsum(\"ibhd,jbhd->bhij\", Q, K) / math.sqrt(self.d_head)\n",
        "      # shape of scaled_QK is (batch_size, num_heads, seq_len, seq_len)\n",
        "      #TODO\n",
        "      k = seq_len // 2\n",
        "      mask = torch.triu(torch.ones_like(scaled_QK), diagonal = 1)\n",
        "      mask[:, :, :, :k] = 0\n",
        "      mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "      scaled_QK = scaled_QK + mask\n",
        "      #ENDTODO\n",
        "      weights = F.softmax(scaled_QK, -1)\n",
        "      attention = torch.einsum(\"bhij,jbhd->ibhd\", weights, V)\n",
        "\n",
        "      result = self.W_O(attention.reshape(seq_len, batch_size,self.num_heads * self.d_head))\n",
        "\n",
        "      return result, weights"
      ],
      "metadata": {
        "id": "ENFiK_cTeDM0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your solution\n",
        "d_model = 2\n",
        "num_heads= 3\n",
        "d_head = 3\n",
        "k = 3\n",
        "batch_size = 10\n",
        "\n",
        "mha = MultiHeadAttention(d_model, num_heads, d_head)\n",
        "batched_x= torch.randn((2*k, batch_size, d_model))\n",
        "with torch.no_grad():\n",
        "  result, weights = mha(batched_x)\n",
        "print(\"Result:\", result)\n",
        "print(\"Weights:\", weights)"
      ],
      "metadata": {
        "id": "GDQ0a57NeB-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747d4d83-5245-4623-cc27-47588b498ef4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: tensor([[[-0.0738, -0.7158],\n",
            "         [-0.2962, -0.8647],\n",
            "         [-0.0551, -0.6896],\n",
            "         [-0.0719, -0.7750],\n",
            "         [-0.0245, -0.6096],\n",
            "         [-0.2812, -0.8499],\n",
            "         [-0.0889, -0.6623],\n",
            "         [ 0.0045, -0.6231],\n",
            "         [ 0.1234, -0.6266],\n",
            "         [ 0.1969, -0.4433]],\n",
            "\n",
            "        [[-0.0864, -0.7064],\n",
            "         [-0.3097, -0.8725],\n",
            "         [-0.0520, -0.7458],\n",
            "         [-0.0635, -0.7501],\n",
            "         [-0.0246, -0.6095],\n",
            "         [-0.2700, -0.8494],\n",
            "         [-0.0617, -0.6447],\n",
            "         [ 0.0098, -0.6296],\n",
            "         [ 0.0193, -0.5692],\n",
            "         [ 0.2157, -0.4728]],\n",
            "\n",
            "        [[-0.0749, -0.7412],\n",
            "         [-0.2959, -0.8738],\n",
            "         [-0.0695, -0.7749],\n",
            "         [-0.1043, -0.6927],\n",
            "         [-0.0245, -0.6094],\n",
            "         [-0.2702, -0.8395],\n",
            "         [-0.0772, -0.6421],\n",
            "         [ 0.0099, -0.6196],\n",
            "         [ 0.0804, -0.5689],\n",
            "         [ 0.2238, -0.4892]],\n",
            "\n",
            "        [[-0.0244, -0.6709],\n",
            "         [-0.2381, -0.8245],\n",
            "         [-0.0379, -0.6989],\n",
            "         [-0.0304, -0.6932],\n",
            "         [ 0.0208, -0.6057],\n",
            "         [-0.2133, -0.7957],\n",
            "         [-0.0352, -0.6195],\n",
            "         [-0.0263, -0.6545],\n",
            "         [-0.0173, -0.7073],\n",
            "         [ 0.1651, -0.5136]],\n",
            "\n",
            "        [[-0.0089, -0.6303],\n",
            "         [-0.2184, -0.8161],\n",
            "         [-0.0591, -0.7458],\n",
            "         [-0.0803, -0.6780],\n",
            "         [-0.0243, -0.6315],\n",
            "         [-0.1969, -0.7713],\n",
            "         [-0.0137, -0.6670],\n",
            "         [-0.0595, -0.6867],\n",
            "         [ 0.0229, -0.6689],\n",
            "         [ 0.0514, -0.5928]],\n",
            "\n",
            "        [[ 0.0395, -0.6313],\n",
            "         [-0.1959, -0.7848],\n",
            "         [-0.0248, -0.6892],\n",
            "         [-0.0058, -0.6486],\n",
            "         [-0.0370, -0.6425],\n",
            "         [-0.1453, -0.7714],\n",
            "         [-0.0029, -0.6077],\n",
            "         [-0.0289, -0.6596],\n",
            "         [ 0.0342, -0.6370],\n",
            "         [-0.0770, -0.6578]]])\n",
            "Weights: tensor([[[[0.2603, 0.2800, 0.4596, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2674, 0.3144, 0.4182, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2417, 0.3120, 0.4463, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1930, 0.2026, 0.3812, 0.2232, 0.0000, 0.0000],\n",
            "          [0.1820, 0.1821, 0.2944, 0.2074, 0.1341, 0.0000],\n",
            "          [0.1434, 0.1427, 0.2913, 0.1747, 0.0911, 0.1568]],\n",
            "\n",
            "         [[0.3250, 0.3389, 0.3361, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2745, 0.2947, 0.4308, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2601, 0.4339, 0.3060, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2676, 0.3097, 0.2029, 0.2198, 0.0000, 0.0000],\n",
            "          [0.1987, 0.1559, 0.2321, 0.2535, 0.1598, 0.0000],\n",
            "          [0.1723, 0.1839, 0.1137, 0.1457, 0.2318, 0.1526]],\n",
            "\n",
            "         [[0.2739, 0.2539, 0.4721, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3139, 0.3278, 0.3583, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2945, 0.3280, 0.3775, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1818, 0.1606, 0.4066, 0.2510, 0.0000, 0.0000],\n",
            "          [0.1807, 0.1579, 0.3073, 0.2335, 0.1206, 0.0000],\n",
            "          [0.1273, 0.1056, 0.3352, 0.1935, 0.0627, 0.1756]]],\n",
            "\n",
            "\n",
            "        [[[0.2599, 0.4917, 0.2484, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2476, 0.5800, 0.1724, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2426, 0.5054, 0.2520, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2230, 0.3658, 0.2426, 0.1686, 0.0000, 0.0000],\n",
            "          [0.1711, 0.3566, 0.1779, 0.1165, 0.1778, 0.0000],\n",
            "          [0.1579, 0.2676, 0.1611, 0.1202, 0.1611, 0.1322]],\n",
            "\n",
            "         [[0.2567, 0.4961, 0.2472, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1439, 0.7428, 0.1133, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2954, 0.5013, 0.2033, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2336, 0.2886, 0.2816, 0.1962, 0.0000, 0.0000],\n",
            "          [0.2022, 0.3425, 0.1389, 0.1778, 0.1386, 0.0000],\n",
            "          [0.1550, 0.2313, 0.1842, 0.1189, 0.1842, 0.1265]],\n",
            "\n",
            "         [[0.3247, 0.3787, 0.2966, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4162, 0.3776, 0.2062, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2836, 0.4038, 0.3126, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2356, 0.2899, 0.2735, 0.2010, 0.0000, 0.0000],\n",
            "          [0.1838, 0.2621, 0.2029, 0.1483, 0.2029, 0.0000],\n",
            "          [0.1647, 0.1909, 0.1687, 0.1515, 0.1687, 0.1554]]],\n",
            "\n",
            "\n",
            "        [[[0.2057, 0.2755, 0.5187, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1795, 0.2527, 0.5678, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1996, 0.2302, 0.5702, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1571, 0.2040, 0.4254, 0.2134, 0.0000, 0.0000],\n",
            "          [0.1105, 0.1373, 0.3389, 0.1539, 0.2594, 0.0000],\n",
            "          [0.0955, 0.1377, 0.2869, 0.1357, 0.2382, 0.1060]],\n",
            "\n",
            "         [[0.2608, 0.3429, 0.3963, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3777, 0.3345, 0.2878, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4038, 0.2494, 0.3468, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2374, 0.2392, 0.2761, 0.2473, 0.0000, 0.0000],\n",
            "          [0.2558, 0.1671, 0.1885, 0.2196, 0.1689, 0.0000],\n",
            "          [0.1648, 0.1777, 0.1526, 0.1636, 0.1626, 0.1788]],\n",
            "\n",
            "         [[0.1822, 0.2804, 0.5374, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1486, 0.2512, 0.6001, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2956, 0.2973, 0.4071, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1588, 0.2220, 0.4044, 0.2149, 0.0000, 0.0000],\n",
            "          [0.1363, 0.1656, 0.2832, 0.1707, 0.2443, 0.0000],\n",
            "          [0.0718, 0.1306, 0.3250, 0.1182, 0.2641, 0.0902]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.4574, 0.2635, 0.2791, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4312, 0.3131, 0.2558, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4176, 0.3101, 0.2723, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3152, 0.1942, 0.2021, 0.2885, 0.0000, 0.0000],\n",
            "          [0.2398, 0.1400, 0.1521, 0.2223, 0.2458, 0.0000],\n",
            "          [0.2085, 0.1539, 0.1290, 0.1750, 0.1928, 0.1408]],\n",
            "\n",
            "         [[0.4558, 0.2523, 0.2919, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3041, 0.3295, 0.3664, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3286, 0.3594, 0.3120, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3131, 0.2036, 0.2016, 0.2817, 0.0000, 0.0000],\n",
            "          [0.2466, 0.1386, 0.1447, 0.2214, 0.2486, 0.0000],\n",
            "          [0.1577, 0.1750, 0.1738, 0.1609, 0.1575, 0.1750]],\n",
            "\n",
            "         [[0.3682, 0.3056, 0.3263, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4149, 0.3539, 0.2312, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3842, 0.3488, 0.2670, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2654, 0.2300, 0.2408, 0.2638, 0.0000, 0.0000],\n",
            "          [0.2041, 0.1738, 0.1966, 0.2115, 0.2141, 0.0000],\n",
            "          [0.2056, 0.1806, 0.1249, 0.1604, 0.1762, 0.1523]]],\n",
            "\n",
            "\n",
            "        [[[0.4842, 0.2569, 0.2589, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4058, 0.3024, 0.2917, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4794, 0.2475, 0.2731, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2514, 0.2107, 0.1742, 0.3637, 0.0000, 0.0000],\n",
            "          [0.2383, 0.1287, 0.1310, 0.2919, 0.2101, 0.0000],\n",
            "          [0.2086, 0.1019, 0.1099, 0.2419, 0.1816, 0.1561]],\n",
            "\n",
            "         [[0.1388, 0.5666, 0.2946, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4687, 0.2401, 0.2912, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3095, 0.3495, 0.3411, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1472, 0.3721, 0.1989, 0.2818, 0.0000, 0.0000],\n",
            "          [0.1188, 0.3530, 0.2122, 0.1776, 0.1385, 0.0000],\n",
            "          [0.1113, 0.2870, 0.1931, 0.1466, 0.1280, 0.1340]],\n",
            "\n",
            "         [[0.5581, 0.1935, 0.2483, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3941, 0.2926, 0.3133, 0.0000, 0.0000, 0.0000],\n",
            "          [0.5613, 0.1842, 0.2544, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2498, 0.2390, 0.2188, 0.2923, 0.0000, 0.0000],\n",
            "          [0.2741, 0.0989, 0.1268, 0.2705, 0.2297, 0.0000],\n",
            "          [0.2420, 0.0699, 0.0975, 0.2270, 0.1960, 0.1675]]],\n",
            "\n",
            "\n",
            "        [[[0.2198, 0.3419, 0.4383, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2067, 0.3393, 0.4541, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2061, 0.3377, 0.4562, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1569, 0.2325, 0.2989, 0.3118, 0.0000, 0.0000],\n",
            "          [0.1205, 0.1510, 0.1814, 0.2085, 0.3386, 0.0000],\n",
            "          [0.0864, 0.0958, 0.1095, 0.1353, 0.2304, 0.3425]],\n",
            "\n",
            "         [[0.2596, 0.3507, 0.3897, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3873, 0.3241, 0.2886, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4538, 0.3021, 0.2441, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3037, 0.2407, 0.2165, 0.2391, 0.0000, 0.0000],\n",
            "          [0.2105, 0.1606, 0.1500, 0.1976, 0.2813, 0.0000],\n",
            "          [0.1176, 0.0868, 0.0830, 0.1251, 0.2293, 0.3581]],\n",
            "\n",
            "         [[0.1601, 0.3372, 0.5027, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1453, 0.3320, 0.5227, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1505, 0.3323, 0.5172, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1365, 0.2363, 0.3226, 0.3046, 0.0000, 0.0000],\n",
            "          [0.1631, 0.1808, 0.1960, 0.2069, 0.2532, 0.0000],\n",
            "          [0.1897, 0.1513, 0.1383, 0.1586, 0.1748, 0.1874]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#some code to think:\n",
        "\n",
        "k = 3\n",
        "A = torch.ones(2*k,2*k)\n",
        "\n",
        "mask = torch.triu(torch.ones_like(A), diagonal = 1)\n",
        "print(mask)\n",
        "mask[:,:k] = 0\n",
        "print(mask)\n",
        "mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "\n",
        "print(mask)\n",
        "print(A+mask)\n",
        "print(torch.nn.functional.softmax(input = (A+mask), dim = -1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig47uNbQO-NH",
        "outputId": "5086d343-3a1e-47d3-bcf1-bc63c0702a9a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[1., 1., 1., -inf, -inf, -inf],\n",
            "        [1., 1., 1., -inf, -inf, -inf],\n",
            "        [1., 1., 1., -inf, -inf, -inf],\n",
            "        [1., 1., 1., 1., -inf, -inf],\n",
            "        [1., 1., 1., 1., 1., -inf],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2fb9EwezQAse"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}