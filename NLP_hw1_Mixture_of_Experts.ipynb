{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Things I did:\n",
        "- Router, and a simple test of it (test is below configs)\n",
        "- MoE (naive version) - I also changed here how the experts look to simplify testing\n",
        "- VectorizedMoE\n",
        "- added \"device\" field to the configuration dict/class\n",
        "- test of whether naive and vectorized version return the same outputs - Here I forcibly set the weights of vectorized one to be the same as in the naive moe\n",
        "- comparison of execution time of both versions of moe\n",
        "- commented out last two 'chapters'"
      ],
      "metadata": {
        "id": "WXTBinAuZ887"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to MoE"
      ],
      "metadata": {
        "id": "m_w7JfijN495"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/2101.03961.pdf\n",
        "\n",
        "https://arxiv.org/pdf/1701.06538.pdf"
      ],
      "metadata": {
        "id": "bF66o0v2Pjww"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_VRo3GfIFp"
      },
      "source": [
        "From Switch Transformer paper:\n",
        "\n",
        ">In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost.\n",
        "\n",
        "A vanilla Transformer block looks like this:\n",
        "\n",
        "```python\n",
        "class ModernTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, up):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, n_heads)\n",
        "        self.mlp = nn.Sequential(\n",
        "            SwishGLU(embed_dim, embed_dim * up),\n",
        "            nn.Linear(embed_dim * up, embed_dim),\n",
        "        )\n",
        "        self.pre_attn_norm = RMSNorm(embed_dim)\n",
        "        self.pre_mlp_norm = RMSNorm(embed_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.pre_attn_norm(x))\n",
        "        x = x + self.mlp(self.pre_mlp_norm(x))\n",
        "        return x\n",
        "```\n",
        "\n",
        "The Mixture-of-Experts layer replaces the MLP layer. Instead of having one MLP layer, we have `num_experts` different MLP layers called *experts*.\n",
        "\n",
        "The idea is to process a contextualized token, by sending it to a subset of experts. In this way we could efficiently increase the number of parameters of the model without affecting computational cost too much.\n",
        "\n",
        "First, the token is fed into *router*, which determines to which experts a token should go to be processed. For computational reasons, there is a fixed limit on:\n",
        "* how many tokens an expert can process, and\n",
        "* by how many experts a token is processed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grading"
      ],
      "metadata": {
        "id": "H2wknptcN1L1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Your task is to implement a Mixture of Experts layer. You can get points for the following subtasks:\n",
        "1.  (5 points) Naive implementation of MoE layer that works with `num_experts_per_token>=1`\n",
        "2.  (5 points) Well-vectorized implementation of MoE layer that works with `num_experts_per_token=1`\n",
        "3.  (5 points) Implementation of a script testing for 1. 2. implementations output equivalence and performance superiority of 2.\n",
        "4.  (5 points) Well-vectorized implementation of MoE layer that works with `num_experts_per_token>=1`\n",
        "5.  (Bonus 5 points) Use Huggingface's Trainer class and compare performance of randomly initialized MoE Transformer and standard Transformer on `https://huggingface.co/datasets/imdb` dataset.\n",
        "\n",
        "20 points scored in this task is equivalent to at least 16% points achievable in this course.\n",
        "\n",
        "Please submit your assignments until 15th of April, 18:00 CET.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FXbTKRIJNfaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rules"
      ],
      "metadata": {
        "id": "Ub9fqv29NrPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- You shouldn't change basic `forward` and `initialization` signatures of the main classes: `Router` and `MoE`. You can add additional arguments with default values.\n",
        "- As an assignment, provide a Jupyter notebook with a short introduction at the top of what has been done and where.\n",
        "- You can add or remove any other classes, though you should keep the behaviour of `MLP` class somehow.\n",
        "- Sensible vectorization is good enough for the maximum amount of points. There is no need to optimize performance to the max, just show that you can identify opportunities for vectorization and you are able to implement complex vectorizations.\n",
        "- If in doubt, direct questions to either Jan Ludziejewski or Juliusz StraszyÅ„ski.\n",
        "- A notebook that is hard to grade (crashing, obfuscated) might be scored for 0 points."
      ],
      "metadata": {
        "id": "KDeviXY3NnhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hints"
      ],
      "metadata": {
        "id": "xiPtbDzMNv1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- First, write a naive implementation, vectorized operations might be hard to analyze for correctness.\n",
        "- You can make randomness deterministic by appropriate torch functions.\n",
        "- If you have a hard time fulfilling fair randomness for token discarding, you can try keeping the earlier tokens."
      ],
      "metadata": {
        "id": "TgojL2cCNiW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "yh3a3zPwfIFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0eba13f-d553-40f1-9eb2-a26dc65c24e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_tb_profiler in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torch_tb_profiler) (2.0.3)\n",
            "Requirement already satisfied: tensorboard!=2.1.0,>=1.15 in /usr/local/lib/python3.10/dist-packages (from torch_tb_profiler) (2.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->torch_tb_profiler) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->torch_tb_profiler) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->torch_tb_profiler) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->torch_tb_profiler) (1.25.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch_tb_profiler einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "XrwQghs3fIFt"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from transformers import PretrainedConfig\n",
        "import torch.nn.functional as F\n",
        "from einops import einsum\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.intermediate_size, config.hidden_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMG2OUC1fIFu"
      },
      "source": [
        "# Router\n",
        "The router is a module which assigns tokens to experts. It answers two questions:\n",
        "1. Which tokens should be assigned to which expert.\n",
        "2. How much weight should be assigned to each expert. The weight is determined by similarity between the token embedding and the expert embedding\n",
        "\n",
        "The following conditions must be satisfied:\n",
        "1. The routing weights must sum to 1 for each token and be non-negative\n",
        "2. A token should have exactly `num_experts_per_token` non-zero weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "Tyie1JaVfIFu"
      },
      "outputs": [],
      "source": [
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, num_experts] - expert routing weights\n",
        "class Router(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts = config.num_experts\n",
        "\n",
        "        self.expert_embeddings = nn.Parameter(torch.randn(self.num_experts, self.hidden_size)).to(config.device)\n",
        "        torch.nn.init.kaiming_uniform_(self.expert_embeddings, nonlinearity='linear')\n",
        "\n",
        "    def forward(self, x):\n",
        "        #my code{\n",
        "        dot = torch.einsum(\"bsh,eh->bse\", x, self.expert_embeddings)\n",
        "        top_k_out = torch.topk(dot, k=self.num_experts_per_token)\n",
        "        top_k = (float(\"-inf\") * torch.ones_like(dot)).scatter_(dim=-1, index=top_k_out.indices, src=top_k_out.values)\n",
        "        res = torch.nn.functional.softmax(top_k, dim=-1)\n",
        "        return res\n",
        "        #}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MoE"
      ],
      "metadata": {
        "id": "I2f_JMjMX5od"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJph0HYCfIFv"
      },
      "source": [
        "The MoE module is a module which wraps around a set of expert modules and a router module.\n",
        "\n",
        "It takes input embeddings and routes them to the experts.\n",
        "\n",
        "Each token is processed individually by a subset of experts.\n",
        "\n",
        "The output token embedding is a weighted sum of the expert outputs.\n",
        "\n",
        "The weights are determined by the router module.\n",
        "\n",
        "The subset of experts is determined by non-zero weights in the routing output.\n",
        "\n",
        "Additionally each expert might process at most `expert_capacity = ceil((batch_size * seq_len) / num_experts * capacity_factor)` tokens\n",
        "\n",
        "Superfluous tokens to be discarded by a particular expert should be selected uniformly at random.\n",
        "\n",
        "Discarding should be equivalent to setting the appropriate routing weight to 0, while other weights remain the same.\n",
        "\n",
        "This means that a token is processed by at most num_experts_per_token experts with a sum of weights of at most 1.\n",
        "\n",
        "Specifically, this could mean that a token is processed by 0 experts - in this case the resulting embedding should be a zero tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "XYWT1tWwfIFv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, hidden_size] - output embeddings\n",
        "class MoE(nn.Module):\n",
        "    \"\"\"version which takes first not random tokens up to expert_capacity\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts = config.num_experts\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.capacity_factor = config.capacity_factor\n",
        "\n",
        "        # You can change experts representation if you want\n",
        "        #self.experts = nn.ModuleList([MLP(config) for _ in range(self.num_experts)])\n",
        "        #not as above but as below instead so as to compare more easily with the vectorized version\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "        self.first_linear = nn.Parameter(torch.randn(self.num_experts, self.intermediate_size, self.hidden_size)).to(config.device)\n",
        "        torch.nn.init.kaiming_uniform_(self.first_linear, nonlinearity='linear')\n",
        "        self.second_linear = nn.Parameter(torch.randn(self.num_experts, self.hidden_size, self.intermediate_size)).to(config.device)\n",
        "        torch.nn.init.kaiming_uniform_(self.second_linear, nonlinearity='linear')\n",
        "\n",
        "        self.router = Router(config)\n",
        "\n",
        "    def experts(self, nr, data):\n",
        "        return self.second_linear[nr] @ torch.nn.functional.relu(self.first_linear[nr] @ data)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        #assert hidden_size == self.hidden_size\n",
        "        expert_capacity = math.ceil(batch_size * seq_len / self.num_experts * self.capacity_factor)\n",
        "\n",
        "        result = torch.zeros_like(x)\n",
        "        weights = self.router(x)\n",
        "\n",
        "        for e in range(self.num_experts):\n",
        "            used = 0\n",
        "            for i in range(batch_size):\n",
        "                for j in range(seq_len):\n",
        "                    if (used < expert_capacity):\n",
        "                        w = weights[i, j, e]\n",
        "                        if(w > 0):\n",
        "                            used += 1\n",
        "                            result[i, j] += (w * self.experts(e, x[i, j]))\n",
        "\n",
        "\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, hidden_size] - output embeddings\n",
        "class VectorizedMoE(nn.Module):\n",
        "    \"\"\"version which takes first not random tokens up to expert_capacity\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts = config.num_experts\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.capacity_factor = config.capacity_factor\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "\n",
        "        # You can change experts representation if you want\n",
        "        self.first_linear = nn.Parameter(torch.randn(self.num_experts, self.intermediate_size, self.hidden_size)).to(config.device)\n",
        "        torch.nn.init.kaiming_uniform_(self.first_linear, nonlinearity='linear')\n",
        "        self.second_linear = nn.Parameter(torch.randn(self.num_experts, self.hidden_size, self.intermediate_size)).to(config.device)\n",
        "        torch.nn.init.kaiming_uniform_(self.second_linear, nonlinearity='linear')\n",
        "\n",
        "        self.router = Router(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        #assert hidden_size == self.hidden_size\n",
        "        expert_capacity = math.ceil(batch_size * seq_len / self.num_experts * self.capacity_factor)\n",
        "\n",
        "        weights = self.router(x) #[batch_size, seq_len, num_experts]\n",
        "\n",
        "        experts_where_ones = torch.where((weights <= 0), 0, 1) #ceiling of weights\n",
        "        experts_where_ones = torch.reshape(experts_where_ones, shape=(-1, self.num_experts)) #[num_of_tokens, num_experts]\n",
        "        capacity_aware_ones = torch.where((torch.cumsum(experts_where_ones, dim= 0) <= expert_capacity), input = experts_where_ones, other = 0)\n",
        "\n",
        "        # dec_seq = experts_where_ones.shape[0] - torch.arange(experts_where_ones.shape[0]).unsqueeze(dim = 1)\n",
        "        # numbered = (experts_where_ones * dec_seq)\n",
        "        # which = torch.topk(numbered, k=expert_capacity, dim = 0)\n",
        "        capacity_aware_weights = weights.reshape(shape=(-1, self.num_experts)) * capacity_aware_ones\n",
        "        which = torch.topk(capacity_aware_weights, k=expert_capacity, dim = 0)\n",
        "        indices = which.indices.transpose(1,0)\n",
        "        index = indices.reshape((-1))\n",
        "\n",
        "        tokens_for_experts = torch.index_select(input=x.reshape((-1, hidden_size)), dim=0, index=index) #[capacity*num_experts, hidden_size]\n",
        "        tokens_for_experts  = tokens_for_experts.reshape((self.num_experts, expert_capacity, hidden_size))\n",
        "        #now I have the proper input to the \"experts\", which I should process by first layer parameters\n",
        "\n",
        "        intermediate_result = torch.einsum(\"ech,eih->eci\", tokens_for_experts, self.first_linear)\n",
        "        intermediate_result = torch.nn.functional.relu(intermediate_result)\n",
        "        result = torch.einsum(\"eci,ehi->ech\", intermediate_result, self.second_linear)\n",
        "        #now tokens are processed by the \"experts\", I need to multiply by the weights and add them up\n",
        "\n",
        "        w = which.values.transpose(1,0).unsqueeze(-1)\n",
        "\n",
        "        result = result * w\n",
        "\n",
        "        final_result = torch.zeros_like(x).reshape((-1, hidden_size)).index_add_(dim = 0, index=index, source = result.reshape((-1, hidden_size)))\n",
        "\n",
        "        return final_result.reshape(x.shape)"
      ],
      "metadata": {
        "id": "oijb9KVoXRCE"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDRxR0HafIFw"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3qW0kGyOTRw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "W_0dW5DsfIFw"
      },
      "outputs": [],
      "source": [
        "base_config = dict(\n",
        "    vocab_size=5000,\n",
        "    max_position_embeddings=256,\n",
        "    num_attention_heads=8,\n",
        "    num_hidden_layers=4,\n",
        "    hidden_dropout_prob=0.1,\n",
        "    hidden_size=128,\n",
        "    intermediate_size=512,\n",
        "    num_labels=2,\n",
        "    device = DEVICE #I added this one\n",
        ")\n",
        "\n",
        "standard_config = PretrainedConfig(\n",
        "    **base_config,\n",
        "    ff_cls=MLP\n",
        ")\n",
        "\n",
        "moe_config = PretrainedConfig(\n",
        "    **base_config,\n",
        "    num_experts=4,\n",
        "    capacity_factor=2.0,\n",
        "    num_experts_per_token=1,\n",
        "    ff_cls=MoE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Simple test of router:"
      ],
      "metadata": {
        "id": "HqvKdbAKQdSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#simple_test - there should be 2 nonzero entries in each row and they should add up to one\n",
        "(batch_size, seq_len, hidden_size) = (3, 5, moe_config.hidden_size)\n",
        "\n",
        "input = torch.randn((batch_size, seq_len, hidden_size))\n",
        "moe_config.num_experts_per_token = 2\n",
        "r = Router(moe_config)\n",
        "out = r(input)\n",
        "\n",
        "#do they add up to one?\n",
        "s = torch.sum(out, dim = -1)\n",
        "assert torch.all(torch.isclose(torch.ones_like(s), s)) #yes\n",
        "\n",
        "#visual inspection:\n",
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlC1wHcyq27H",
        "outputId": "aa003b00-9bb1-4b8c-f645-69e9162ea5c7"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.1953, 0.8047, 0.0000],\n",
              "        [0.8655, 0.0000, 0.0000, 0.1345],\n",
              "        [0.5185, 0.0000, 0.4815, 0.0000],\n",
              "        [0.0000, 0.0613, 0.0000, 0.9387],\n",
              "        [0.0000, 0.5570, 0.0000, 0.4430]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing naive and vectorized versions:\n",
        "\"settings can be changed by passing a different config\""
      ],
      "metadata": {
        "id": "_qbnzyTuMqEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_result(config=moe_config, num=1, batch_size=20, seq_len=50):\n",
        "    vectorized_moe = VectorizedMoE(config)\n",
        "    naive_moe = MoE(config)\n",
        "    #setting weights of vectorized moe to the naive one's\n",
        "    vectorized_moe.first_linear = naive_moe.first_linear\n",
        "    vectorized_moe.second_linear = naive_moe.second_linear\n",
        "    vectorized_moe.router = naive_moe.router\n",
        "\n",
        "    hidden_size = config.hidden_size\n",
        "    input = torch.randn((batch_size, seq_len, hidden_size))\n",
        "\n",
        "    v = vectorized_moe(input)\n",
        "    n = naive_moe(input)\n",
        "\n",
        "    for i in range(num-1):\n",
        "        v = vectorized_moe(v)\n",
        "        n = naive_moe(n)\n",
        "    return torch.all(torch.isclose(n, v))\n",
        "\n",
        "compare_result(num=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jELCAXPwNNNH",
        "outputId": "fdd03d03-8563-4beb-8568-a6ec88db2188"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def compare_time(config=moe_config, num=5, batch_size=20, seq_len=50):\n",
        "    vectorized_moe = VectorizedMoE(config)\n",
        "    naive_moe = MoE(config)\n",
        "\n",
        "    hidden_size = config.hidden_size\n",
        "    input = torch.randn((batch_size, seq_len, hidden_size))\n",
        "\n",
        "    durationV = 0\n",
        "    for i in range(num):\n",
        "        start_time = time.time()\n",
        "        vectorized_moe(input)\n",
        "        end_time = time.time()\n",
        "        durationV += end_time-start_time\n",
        "\n",
        "    durationN = 0\n",
        "    for i in range(num):\n",
        "        start_time = time.time()\n",
        "        naive_moe(input)\n",
        "        end_time = time.time()\n",
        "        durationN += end_time-start_time\n",
        "\n",
        "    return durationN/num, durationV/num\n",
        "compare_time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtIyuQsyVHPN",
        "outputId": "fad5b329-c6d5-4565-9854-b1674f02fc5f"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3744971752166748, 0.012471437454223633)"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "or using timeit:"
      ],
      "metadata": {
        "id": "_M9GXlQ4ZIoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "vectorized_moe = VectorizedMoE(config)\n",
        "naive_moe = MoE(config)\n",
        "\n",
        "(batch_size, seq_len, hidden_size) = (20, 50, config.hidden_size)\n",
        "input = torch.randn((batch_size, seq_len, hidden_size))"
      ],
      "metadata": {
        "id": "Y_TZFjVEW2Gg"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit vectorized_moe(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTN_Y8k3W6vC",
        "outputId": "34a3895f-103f-46c2-e883-05a1fb283aad"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.07 ms Â± 186 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit naive_moe(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gkn21_8XF_m",
        "outputId": "7e600b11-111d-405d-b7c6-6cc5fc405e43"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.81 ms Â± 132 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abAV-rA3fIFy"
      },
      "source": [
        "# Basic Transformer-related classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "gXd1lstpfIFy"
      },
      "outputs": [],
      "source": [
        "# from einops import rearrange\n",
        "\n",
        "# class Embedding(nn.Module):\n",
        "#   def __init__(self, config):\n",
        "#     super(Embedding, self).__init__()\n",
        "#     self.word_embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "#     self.pos_embed = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "#     self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     batch_size, seq_length = x.shape\n",
        "#     device = x.device\n",
        "#     positions = torch.arange(0, seq_length).expand(\n",
        "#         batch_size, seq_length).to(device)\n",
        "#     embedding = self.word_embed(x) + self.pos_embed(positions)\n",
        "#     return self.dropout(embedding)\n",
        "\n",
        "\n",
        "# class MHSelfAttention(nn.Module):\n",
        "#     def __init__(self, config: PretrainedConfig):\n",
        "#         super(MHSelfAttention, self).__init__()\n",
        "#         self.num_attention_heads = config.num_attention_heads\n",
        "#         self.hidden_size = config.hidden_size\n",
        "#         self.head_size = self.hidden_size // self.num_attention_heads\n",
        "#         self.num_attention_heads = config.num_attention_heads\n",
        "#         self.qkv = nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=False)\n",
        "\n",
        "#     def forward(self, embeddings):\n",
        "#         batch_size, seq_length, hidden_size = embeddings.size()\n",
        "\n",
        "#         result = self.qkv(embeddings)\n",
        "#         q, k, v = rearrange(result, 'b s (qkv nah hdsz) -> qkv b nah s hdsz', nah=self.num_attention_heads, qkv=3).unbind(0)\n",
        "\n",
        "#         attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
        "#         attention_scores = attention_scores / math.sqrt(hidden_size)\n",
        "#         attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "#         contextualized_layer = torch.matmul(attention_probs, v)\n",
        "\n",
        "#         outputs = rearrange(contextualized_layer, 'b nah s hdsz -> b s (nah hdsz)')\n",
        "#         return outputs\n",
        "\n",
        "# class TransformerBlock(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super().__init__()\n",
        "#         self.attention = MHSelfAttention(config)\n",
        "#         self.norm1 = nn.LayerNorm(config.hidden_size)\n",
        "#         self.norm2 = nn.LayerNorm(config.hidden_size)\n",
        "#         self.intermediate = config.ff_cls(config)\n",
        "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x =  x + self.norm1(self.dropout(self.attention(x)))\n",
        "#         x =  x + self.norm2(self.dropout(self.intermediate(x)))\n",
        "#         return x\n",
        "\n",
        "# class TransformerClassifier(nn.Module):\n",
        "#     def __init__(self, config):\n",
        "#         super().__init__()\n",
        "#         self.embeddings = Embedding(config)\n",
        "#         self.layer = nn.Sequential(*[TransformerBlock(config) for _ in range(config.num_hidden_layers)])\n",
        "#         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, labels=None):\n",
        "#         embedding_output = self.embeddings(input_ids)\n",
        "#         encoding = self.layer(embedding_output)\n",
        "#         pooled_encoding = encoding.mean(dim=1)\n",
        "#         logits = self.classifier(pooled_encoding)\n",
        "#         loss = F.cross_entropy(logits, labels) if labels is not None else None\n",
        "#         return {\n",
        "#             'loss': loss,\n",
        "#             'logits': logits,\n",
        "#         }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg4Jm-uFfIFy"
      },
      "source": [
        "# Tokenizer training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # install datasets\n",
        "# !pip install datasets"
      ],
      "metadata": {
        "id": "SLhBdV9oS7m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "FdqQL-HNfIFz"
      },
      "outputs": [],
      "source": [
        "# from tokenizers import ByteLevelBPETokenizer\n",
        "# from datasets import load_dataset\n",
        "# from tokenizers.processors import BertProcessing\n",
        "\n",
        "# dataset = load_dataset('imdb')\n",
        "\n",
        "# tokenizer = ByteLevelBPETokenizer()\n",
        "# tokenizer.train_from_iterator(\n",
        "#     dataset['train']['text'],\n",
        "#     vocab_size=base_config['vocab_size'],\n",
        "#     special_tokens=[\"<s>\", \"</s>\", \"<pad>\"],\n",
        "#     min_frequency=2\n",
        "# )\n",
        "# tokenizer.post_processor = BertProcessing(\n",
        "#     (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "#     (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "# )\n",
        "\n",
        "# tokenizer.enable_truncation(max_length=base_config['max_position_embeddings'])\n",
        "# tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"<pad>\"), pad_token=\"<pad>\", length=base_config['max_position_embeddings'])\n",
        "# tokenizer.model_max_length = base_config['max_position_embeddings']\n",
        "# tokenizer.pad_token = \"<pad>\"\n",
        "\n",
        "# from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# def tokenize(row):\n",
        "#     return {\n",
        "#         'input_ids': tokenizer.encode(row['text']).ids,\n",
        "#     }\n",
        "\n",
        "# tokenized_dataset = dataset.map(tokenize)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml-training",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m_w7JfijN495",
        "H2wknptcN1L1",
        "xiPtbDzMNv1y",
        "UMG2OUC1fIFu",
        "abAV-rA3fIFy",
        "bg4Jm-uFfIFy"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}